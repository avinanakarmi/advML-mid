{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "executionInfo": {
     "elapsed": 514172,
     "status": "error",
     "timestamp": 1743574204798,
     "user": {
      "displayName": "Avina Nakarmi",
      "userId": "13575608521010464654"
     },
     "user_tz": 240
    },
    "id": "idlcW2bMn4EV",
    "outputId": "12c7108a-742e-4824-f953-7d15b72b3214"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved highlighted image for beijing to ../satelite_image_data/beijing_change.png\n",
      "Saved highlighted image for philipines to ../satelite_image_data/philipines_change.png\n",
      "Saved highlighted image for ktm to ../satelite_image_data/ktm_change.png\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from contrastive import CPCA\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "\n",
    "path = \"../satelite_image_data/\"\n",
    "\n",
    "images = [\n",
    "    {\"before\": \"beijing_old\", \"after\": \"beijing_new\"},\n",
    "    {\"before\": \"philipines_old\", \"after\": \"philipines_new\"},\n",
    "    {\"before\": \"ktm_old\", \"after\": \"ktm_new\"}\n",
    "]\n",
    "\n",
    "for image_pair in images:\n",
    "    befnaf = {}\n",
    "    before = image_pair[\"before\"]\n",
    "    after = image_pair[\"after\"]\n",
    "\n",
    "    image_before = Image.open(f\"{path}{before}.png\").convert(\"L\")\n",
    "    image_after  = Image.open(f\"{path}{after}.png\").convert(\"L\")\n",
    "\n",
    "    original_after = image_before.copy()\n",
    "\n",
    "    resized_dims = (min(image_before.size[0], image_after.size[0]), min(image_before.size[1], image_after.size[1]))\n",
    "    image_before = image_before.resize(resized_dims)\n",
    "    image_after  = image_after.resize(resized_dims)\n",
    "\n",
    "    before_array = np.array(image_before)\n",
    "    after_array  = np.array(image_after)\n",
    "\n",
    "    befnaf[before.split(\"_\")[0]] = [before_array, after_array, original_after]\n",
    "\n",
    "    for key, val in befnaf.items():\n",
    "      mdl = CPCA()\n",
    "      fg = val[0]  # \"before\" image (resized, B&W)\n",
    "      bg = val[1]  # \"after\" image (resized, B&W)\n",
    "      original_after = val[2]  # PIL image at original dimensions (B&W)\n",
    "\n",
    "      # Create active labels for CPCA (one per row)\n",
    "      active_labels = list(range(fg.shape[0]))\n",
    "\n",
    "      _ = mdl.fit_transform(fg, bg, plot=False, active_labels=active_labels)\n",
    "\n",
    "      # Compute CPCA projection for the background (resized)\n",
    "      try:\n",
    "          proj_after = mdl.transform(bg)\n",
    "      except Exception:\n",
    "          proj_after = np.dot(bg, mdl.components_.T)\n",
    "\n",
    "      if isinstance(proj_after, list):\n",
    "          proj_after = proj_after[0]\n",
    "      # Use the real part of the first component as row scores as it highlights the most change\n",
    "      row_scores = np.real(proj_after[:, 0])\n",
    "\n",
    "      # Run CPCA on the transposed images (treat columns as samples)\n",
    "      mdl2 = CPCA()\n",
    "      _ = mdl2.fit_transform(fg.T, bg.T, plot=False)\n",
    "      try:\n",
    "          proj_after_T = mdl2.transform(bg.T)\n",
    "      except Exception:\n",
    "          proj_after_T = np.dot(bg.T, mdl2.components_.T)\n",
    "\n",
    "      if isinstance(proj_after_T, list):\n",
    "          proj_after_T = proj_after_T[0]\n",
    "      col_scores = np.real(proj_after_T[:, 0])\n",
    "\n",
    "      # Combine row and column scores: outer product gives a change score per pixel on the resized image.\n",
    "      combined_score = np.outer(row_scores, col_scores)\n",
    "\n",
    "      norm_combined = (combined_score - combined_score.min()) / (combined_score.max() - combined_score.min())\n",
    "\n",
    "      # Create a binary mask: highlight pixels in the top 10% of change score.\n",
    "      threshold = np.percentile(norm_combined, 90)\n",
    "      mask = norm_combined > threshold  # mask has shape equal to resized image\n",
    "\n",
    "      orig_width, orig_height = original_after.size\n",
    "      zoom_factors = (orig_height / mask.shape[0], orig_width / mask.shape[1])\n",
    "      mask_upscaled = zoom(mask.astype(float), zoom_factors, order=0) >= 0.5\n",
    "\n",
    "      orig_after_rgb = np.array(original_after.convert(\"RGB\"))\n",
    "\n",
    "      highlighted = orig_after_rgb.copy()\n",
    "      highlighted[mask_upscaled] = [28, 201, 115]\n",
    "\n",
    "      output_path = f\"{path}{key}_change.png\"\n",
    "      out_img = Image.fromarray(highlighted)\n",
    "      out_img.save(output_path)\n",
    "      print(f\"Saved highlighted image for {key} to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 194377,
     "status": "ok",
     "timestamp": 1743574438040,
     "user": {
      "displayName": "Avina Nakarmi",
      "userId": "13575608521010464654"
     },
     "user_tz": 240
    },
    "id": "mQUjsVqow7jQ",
    "outputId": "87c4fc89-13b2-47f1-ea68-c60770d213a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved highlighted image for beijing to ../satelite_image_data/beijing_mag_of_change.png\n",
      "Saved highlighted image for philipines to ../satelite_image_data/philipines_mag_of_change.png\n",
      "Saved highlighted image for ktm to ../satelite_image_data/ktm_mag_of_change.png\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from contrastive import CPCA\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "\n",
    "path = \"../satelite_image_data/\"\n",
    "\n",
    "images = [\n",
    "    {\"before\": \"beijing_old\", \"after\": \"beijing_new\"},\n",
    "    {\"before\": \"philipines_old\", \"after\": \"philipines_new\"},\n",
    "    {\"before\": \"ktm_old\", \"after\": \"ktm_new\"}\n",
    "]\n",
    "\n",
    "for image_pair in images:\n",
    "    befnaf = {}\n",
    "    before = image_pair[\"before\"]\n",
    "    after = image_pair[\"after\"]\n",
    "\n",
    "    image_before = Image.open(f\"{path}{before}.png\").convert(\"L\")\n",
    "    image_after  = Image.open(f\"{path}{after}.png\").convert(\"L\")\n",
    "\n",
    "    original_after = image_before.copy()\n",
    "\n",
    "    resized_dims = (min(image_before.size[0], image_after.size[0]), min(image_before.size[1], image_after.size[1]))\n",
    "    image_before = image_before.resize(resized_dims)\n",
    "    image_after  = image_after.resize(resized_dims)\n",
    "\n",
    "    before_array = np.array(image_before)\n",
    "    after_array  = np.array(image_after)\n",
    "\n",
    "    befnaf[before.split(\"_\")[0]] = [before_array, after_array, original_after]\n",
    "\n",
    "    for key, val in befnaf.items():\n",
    "        mdl = CPCA()\n",
    "        fg = val[0]  # \"before\" image (resized)\n",
    "        bg = val[1]  # \"after\" image (resized)\n",
    "        original_after = val[2]  # PIL image at original dimensions\n",
    "\n",
    "        active_labels = list(range(fg.shape[0]))\n",
    "\n",
    "        _ = mdl.fit_transform(fg, bg, plot=False, active_labels=active_labels)\n",
    "\n",
    "        try:\n",
    "            proj_after = mdl.transform(bg)\n",
    "        except Exception:\n",
    "            proj_after = np.dot(bg, mdl.components_.T)\n",
    "\n",
    "        if isinstance(proj_after, list):\n",
    "            proj_after = proj_after[0]\n",
    "        row_scores = np.real(proj_after[:, 0])\n",
    "\n",
    "        mdl2 = CPCA()\n",
    "        _ = mdl2.fit_transform(fg.T, bg.T, plot=False)\n",
    "\n",
    "        try:\n",
    "            proj_after_T = mdl2.transform(bg.T)\n",
    "        except Exception:\n",
    "            proj_after_T = np.dot(bg.T, mdl2.components_.T)\n",
    "\n",
    "        if isinstance(proj_after_T, list):\n",
    "            proj_after_T = proj_after_T[0]\n",
    "        col_scores = np.real(proj_after_T[:, 0])\n",
    "\n",
    "        combined_score = np.outer(row_scores, col_scores)\n",
    "\n",
    "        # Normalize the combined score to get continous change\n",
    "        norm_combined = (combined_score - combined_score.min()) / (combined_score.max() - combined_score.min())\n",
    "\n",
    "        orig_width, orig_height = original_after.size\n",
    "        zoom_factors = (orig_height / norm_combined.shape[0], orig_width / norm_combined.shape[1])\n",
    "        alpha_mask = zoom(norm_combined, zoom_factors, order=1)\n",
    "\n",
    "        orig_after_rgb = np.array(original_after.convert(\"RGB\"))\n",
    "\n",
    "        red_overlay = np.full(orig_after_rgb.shape, [28, 201, 115], dtype=np.uint8)\n",
    "\n",
    "        alpha_expanded = alpha_mask[..., np.newaxis]\n",
    "        highlighted = (1 - alpha_expanded) * orig_after_rgb + alpha_expanded * red_overlay\n",
    "        highlighted = highlighted.astype(np.uint8)\n",
    "\n",
    "        output_path = f\"{path}{key}_mag_of_change.png\"\n",
    "        out_img = Image.fromarray(highlighted)\n",
    "        out_img.save(output_path)\n",
    "        print(f\"Saved highlighted image for {key} to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
